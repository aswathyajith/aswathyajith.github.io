<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Counting in Sight, Words, and Thought</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        nav {
            background: white;
            padding: 1rem 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav a {
            color: #333;
            text-decoration: none;
            margin-right: 2rem;
            font-weight: 500;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #0066cc;
        }

        .container {
            max-width: 1200px;
            margin: 3rem auto;
            padding: 0 2rem;
        }

        article {
            background: white;
            padding: 3rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            color: #1a1a1a;
            line-height: 1.2;
        }

        .meta {
            color: #666;
            font-size: 0.95rem;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid #eee;
        }

        time {
            font-weight: 500;
        }

        article p {
            margin-bottom: 1.5rem;
            font-size: 1.1rem;
            color: #444;
        }

        article h2 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: #2a2a2a;
        }

        /* Make ordered lists fit narrower sections */
        article ol {
            max-width: 900px; /* narrower than article width */
            margin-left: auto;
            margin-right: auto;
            padding-left: 1.25rem; /* keep numbering readable */
            padding-right: 1.25rem;
        }

        article ol li {
            margin-bottom: 0.5rem;
        }

        article a {
            color: #0066cc;
            text-decoration: none;
            border-bottom: 1px solid #0066cc;
        }

        article a:hover {
            color: #0052a3;
            border-bottom-color: #0052a3;
        }

        code {
            background: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        .taxonomy {
            width: 100%;
            height: auto;
        }

        .caption {
            text-align: center;
            font-size: 0.9rem;
            color: #666;
            margin-top: 0.5rem;
            margin-bottom: 2rem;
            font-style: italic;
        }

        .image-block {
            display: flex;
            flex-direction: column;
            gap: 2rem;
            margin: 2rem 0;
            align-items: center;
        }

        .image-block .image-container {
            width: 100%;
            text-align: center;
        }

        .image-block img {
            width: 100%;
            height: auto;
            border-radius: 4px;
        }

        .image-block.narrow .image-container {
            max-width: 600px;
            margin: 0 auto;
        }

        .image-block .caption {
            margin-top: 0.5rem;
            margin-bottom: 0;
            font-size: 0.85rem;
        }

        .image-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-template-rows: auto auto;
            gap: 1rem;
            transform: scale(0.6);
            transform-origin: center;
            margin-top: -13rem;
            margin-bottom: -10rem;
        }

        .image-grid .image-container:first-child {
            grid-column: 1;
            grid-row: 1;
        }

        .image-grid .image-container:nth-child(2) {
            grid-column: 2;
            grid-row: 1;
        }

        .image-grid .image-container:nth-child(3) {
            grid-column: 1 / -1;
            grid-row: 2;
            max-width: 80%;
            margin: 0 auto;
        }

        .image-grid img {
            width: 100%;
            height: auto;
            border-radius: 4px;
        }

        .image-grid .caption {
            margin-top: 0.5rem;
            margin-bottom: 0;
            font-size: 1.5rem;
            text-align: center;
        }

        @media (max-width: 768px) {
            .image-block {
                flex-direction: column;
                gap: 1rem;
            }

            .image-grid {
                grid-template-columns: 1fr;
                grid-template-rows: auto auto auto;
                gap: 1rem;
            }

            .image-grid .image-container:first-child {
                grid-column: 1;
                grid-row: 1;
            }

            .image-grid .image-container:nth-child(2) {
                grid-column: 1;
                grid-row: 2;
            }

            .image-grid .image-container:nth-child(3) {
                grid-column: 1;
                grid-row: 3;
            }
        }

        .subtitle {
            font-size: 1.2rem;
            color: #666;
            margin-bottom: 2rem;
            font-style: italic;
        }

        blockquote {
            border-left: 4px solid #0066cc;
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            color: #555;
            font-style: italic;
        }

        @media (max-width: 768px) {
            article {
                padding: 2rem 1.5rem;
            }

            h1 {
                font-size: 2rem;
            }

            .container {
                margin: 1.5rem auto;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html">Home</a>
        <a href="index.html">Blog</a>
        <a href="../about.html">About</a>
    </nav>

    <div class="container">
        <article>
            <h1>Counting in Sight, Word, and Thought </h1>
            <div class="subtitle">
                <p>A Study of Count Alignment in Multi-modal Text-Vision Models</p>
            </div>
            <div class="meta">
                <time datetime="2024-10-09">October 9, 2025</time>
            </div>
            <h2>Abstract</h2>
            <p>
                Multi-modal text-vision models have shown remarkable capabilities in generating and editing images with text prompts. However, their ability to process and generate numerical data is still under-explored. This post aims to explore the counting abilities of multi-modal text-vision models. 
            </p>
            <h2>1 Introduction</h2>
            <p>
                <i>Give-N tasks</i> and <i>How-Many tasks</i> are commonly used conceptual measures in cognitive science to evaluate cardinality understanding in children. The <i>How-Many task</i> tests an understanding of the ability to count the number of objects in a given set. In the <i>Give-N task</i>, children are presented with a pile of items and asked to construct a subset of objects of a specific size. <i>Give-N tasks</i> are considered to be more difficult and a more comprehensive measure of cardinality understanding. However, despite the performance differences across the two tasks, covariance between them can be used to test whether they measure the same construct [<a href="https://www.sciencedirect.com/science/article/pii/S0885200623001151/">O'Rear et al., 2023</a>].
                In this post, I will do a deep dive into the performance of multi-modal text-vision models on these tasks. 
            </p>
             <!-- Two-image block -->
             <div class="image-block narrow">
                 <div class="image-container">
                     <img src="figs/orange_3.png" alt="3 oranges.">
                     <p class="caption"><strong>Figure 1a:</strong> Image generated by OmniGen2 for the prompt <strong>3 oranges.</strong></p>
                 </div>
                 <div class="image-container">
                     <img src="figs/sheep_3.png" alt="3 sheep.">
                     <p class="caption"><strong>Figure 1b:</strong> Image generated by OmniGen2 for the prompt <strong>3 sheep.</strong></p>
                 </div>
             </div>
            
            <p>
            To analyze the performance of models on this task, we first define a taxonomy categorizing the different countable noun forms based on morphological and semantic features.
            </p>
            <!-- Two-image block -->
            <div class="image-block">
                <div class="image-container">
                    <img src="figs/morphological_taxonomy.png" alt="Morphological taxonomy.">
                    <p class="caption"><strong>Figure 2a:</strong> Taxonomy of countable noun forms based on morphological features.</p>
                </div>
                <div class="image-container">
                    <img src="figs/semantic_taxonomy.png" alt="Semantic taxonomy.">
                    <p class="caption"><strong>Figure 2b:</strong> Taxonomy of countable noun forms based on semantic features (under construction).</p>
                </div>
            </div>
            

            <h2>2  Related Work</h2>

            <p>
                Recent work evaluating generative models for counting skills have found that vision-text models possess <i>some</i> ability to generate images with the correct number of objects (<i>Give-n</i> task), but can struggle when presented with unusual descriptive modifiers [<a href="https://cocosci.princeton.edu/papers/rane2024count.pdf">Rane et al. (2024)</a>]. Further, their performance degrades rapidly for cardinalities greater than 5. Other works have investigated the ability of vision-text models to count objects in the wild [<a href="https://arxiv.org/pdf/1604.03505">Ma et al. (2025)</a>, <a href="https://arxiv.org/pdf/1810.12440">Acharya et al. (2024)</a>] (<i>How Many?</i> task).
            </p>

            <h2>3 Experimental Framework</h2>

            <p>
                We perform our experiments on the OmniGen2 model to evaluate the numerical cognition abilities of multi-modal text-vision models on simple countable noun entities: 
                <ol>
                    <li>
                        <i>Give-n</i> task: We prompt models with a phrase or instruction to generate an image of a countable noun entity with a pre-specified cardinality. </li>
                    <li>
                        <i>How-Many</i> task: Given a synthetically generated image with single abstract entities, we prompt models to output the cardinality of the entity set in the given image. </li>
                </ol>

            </p>
            <h3>3.1 Evaluation Metrics</h3>
            <p>
                We evaluate model performance on the following metrics:
                <ol>
                    <li><strong>Exact Match Accuracy in Vision Space (acc_v):</strong> We measure the exact match accuracy of the model generated counts by comparing them to that in the input context.</li>
                    <li><strong>Error Rate in Vision Space (err_v):</strong> We measure the model's error rate for an object, number pair by subtracting the model generated count from the input prompt count.</li>
                    <li><strong>Exact Match Accuracy in Text Space (acc_t):</strong> We measure the exact match accuracy of the model generated counts by comparing them to that in the input context.</li>
                    <li><strong>Error Rate in Text Space (err_t):</strong> We measure the model's error rate for an object, number pair by subtracting the model generated count from the input prompt count.</li>
                    <li><strong>Vision-Text Consistency (cons_vt):</strong> We use statistical measures (e.g. Pearson's correlation coefficient) to quantify the consistency between the actual counts of objects in the image generated by the model and the count predicted by the text encoder of the model.</li>
                </ol>
            </p>
            <h3>3.2 Measuring Grouping Behavior</h3>
            <p>
                We categorize the object entities into different categories based on their morphological and semantic properties (see Figure 2). We then evaluate the model outputs for the metrics above grouped by the different object categories.
            </p>
            <h3>3.3 Intervening on the Input Space</h3>
            <p>
                In order to understand the robustness of counting behavior of models on the Give-n and How-Many tasks, we test the model's ability to generate images by intervening on the model's input space. In particular, we study how the following factors affect the consistency in the generated counts of the model in the vision and text space:
                <ol>    
                    <li><strong>Numerical Representation:</strong> We modify the input prompt to use the number in its decimal format and in words (e.g. "3" versus "three").</li>
                    <li><strong>Spatial Arrangement:</strong> We alter the prompt to specify different spatial arrangements for generating the entities in the image (e.g. <i>in a row</i>, <i>in a column</i>, <i>in a grid</i>, <i>in a circle</i>, etc.).</li>
                    <li><strong>Visual Anchoring:</strong> We study the effect of adding a visual anchor to the prompt (<i>a <entity> against a white background</i>, <i><entity> in a natural setting</i>, etc.).</li>
                    <li><strong>Instructive Context:</strong> We measure the impact of presenting the model with an instructional version of the prompt (e.g. <i>generate 3 sheep</i> versus <i>3 sheep</i>).</li>
                    <li><strong>Descriptive Modifiers:</strong> We add natural descriptive modifiers to the noun entity (e.g. <i>3 red apples</i>, <i>3 small sheep</i>, <i>3 large oranges</i>, etc.) to understand the influence of modifiers on model performance.</li>
                </ol>
            </p>

            <h2>4 Results</h2>
            <p>
                As observed by [<a href="https://cocosci.princeton.edu/papers/rane2024count.pdf">Rane et al. (2024)</a>], we find that models exhibit moderate success in generating images of entities with lower cardinalities (1-3). However, the performance degrades rapidly as we increase <i>n</i> to 4 or greater. The following figures show the accuracies, error rates, and distribution counts on the Give-n task. We make the following key observations:
            <div class="image-grid">
                <div class="image-container">
                    <img src="figs/acc.png" alt="Accuracy of model-generated counts.">
                        <p class="caption"><strong>Figure 3a:</strong> Heatmap of exact match (EM) accuracy of model-generated counts on the Give-n task.</p>
                    </div>
                <div class="image-container">
                    <img src="figs/error.png" alt="Error rates on Give-n task.">
                    <p class="caption"><strong>Figure 3b:</strong> Heatmap of error rates between the average model generated count and the input prompt count on the Give-n task.</p>
                </div>
                <div class="image-container">
                    <img src="figs/ridge_plot.png" alt="Give-n count Distribution.">
                    <p class="caption"><strong>Figure 3c:</strong> Distribution of model-generated counts on the Give-n task.</p>
                </div>
            </div>
            <ol>
                <li>The model has a tendency to fail to match the input prompt count by over-producing than under-producing (i.e., generates more objects than what is asked for in the input prompt).</li>
                <li>A bias towards generating objects in counts of 9 (i.e., the model tends to generate 9 objects when the input prompt count is 10).</li>
                <li>Polysemous entities (words that are associated with multiple visual concepts like "fruits", "color", but stemming from the same origin) are harder to generate in accurate counts than others. For example, the model struggles to generate accurate counts of "peach" and "orange" (which can be both a fruit and a color), but is relatively successful at generating a given number of "sheep" and "goats".</li>
                <li>The bias towards producing objects in counts of 9 is amplified in entities that are more difficult to count (i.e. we find that the model significantly produced more images with 9 counts for entities like "orange" and "peach" in which their performance was poorest).</li>
            </ol>
            </p>

            <h2>5 Conclusion</h2>
        </article>
    </div>
</body>
</html>