<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Counting in Sight, Words, and Thought</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        nav {
            background: white;
            padding: 1rem 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav a {
            color: #333;
            text-decoration: none;
            margin-right: 2rem;
            font-weight: 500;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #0066cc;
        }

        .container {
            max-width: 1200px;
            margin: 3rem auto;
            padding: 0 2rem;
        }

        article {
            background: white;
            padding: 3rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            color: #1a1a1a;
            line-height: 1.2;
        }

        .meta {
            color: #666;
            font-size: 0.95rem;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid #eee;
        }

        time {
            font-weight: 500;
        }

        article p {
            margin-bottom: 1.5rem;
            font-size: 1.1rem;
            color: #444;
        }

        article h2 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: #2a2a2a;
        }

        article a {
            color: #0066cc;
            text-decoration: none;
            border-bottom: 1px solid #0066cc;
        }

        article a:hover {
            color: #0052a3;
            border-bottom-color: #0052a3;
        }

        code {
            background: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        .taxonomy {
            width: 100%;
            height: auto;
        }

        .caption {
            text-align: center;
            font-size: 0.9rem;
            color: #666;
            margin-top: 0.5rem;
            margin-bottom: 2rem;
            font-style: italic;
        }

        .image-block {
            display: flex;
            flex-direction: column;
            gap: 2rem;
            margin: 2rem 0;
            align-items: center;
        }

        .image-block .image-container {
            width: 100%;
            text-align: center;
        }

        .image-block img {
            width: 100%;
            height: auto;
            border-radius: 4px;
        }

        .image-block.narrow .image-container {
            max-width: 600px;
            margin: 0 auto;
        }

        .image-block .caption {
            margin-top: 0.5rem;
            margin-bottom: 0;
            font-size: 0.85rem;
        }

        .image-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-template-rows: auto auto;
            gap: 1rem;
            transform: scale(0.65);
            transform-origin: center;
        }

        .image-grid .image-container:first-child {
            grid-column: 1;
            grid-row: 1;
        }

        .image-grid .image-container:nth-child(2) {
            grid-column: 2;
            grid-row: 1;
        }

        .image-grid .image-container:nth-child(3) {
            grid-column: 1 / -1;
            grid-row: 2;
            max-width: 80%;
            margin: 0 auto;
        }

        .image-grid img {
            width: 100%;
            height: auto;
            border-radius: 4px;
        }

        .image-grid .caption {
            margin-top: 0.5rem;
            margin-bottom: 0;
            font-size: 1.5rem;
            text-align: center;
        }

        @media (max-width: 768px) {
            .image-block {
                flex-direction: column;
                gap: 1rem;
            }

            .image-grid {
                grid-template-columns: 1fr;
                grid-template-rows: auto auto auto;
                gap: 1rem;
            }

            .image-grid .image-container:first-child {
                grid-column: 1;
                grid-row: 1;
            }

            .image-grid .image-container:nth-child(2) {
                grid-column: 1;
                grid-row: 2;
            }

            .image-grid .image-container:nth-child(3) {
                grid-column: 1;
                grid-row: 3;
            }
        }

        .subtitle {
            font-size: 1.2rem;
            color: #666;
            margin-bottom: 2rem;
            font-style: italic;
        }

        blockquote {
            border-left: 4px solid #0066cc;
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            color: #555;
            font-style: italic;
        }

        @media (max-width: 768px) {
            article {
                padding: 2rem 1.5rem;
            }

            h1 {
                font-size: 2rem;
            }

            .container {
                margin: 1.5rem auto;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html">Home</a>
        <a href="index.html">Blog</a>
        <a href="../about.html">About</a>
    </nav>

    <div class="container">
        <article>
            <h1>Counting in Sight, Word, and Thought </h1>
            <div class="subtitle">
                <p>A Study of Count Alignment in Multi-modal Text-Vision Models</p>
            </div>
            <div class="meta">
                <time datetime="2024-10-09">October 9, 2025</time>
            </div>
            <h2>Abstract</h2>
            <p>
                Multi-modal text-vision models have shown remarkable capabilities in generating and editing images with text prompts. However, their ability to process and generate numerical data is still under-explored. This post aims to explore the counting abilities of multi-modal text-vision models. 
            </p>
            <h2>Introduction</h2>
            <p>
                <i>Give-N tasks</i> and <i>How-Many tasks</i> are commonly used conceptual measures in cognitive science to evaluate cardinality understanding in children. The <i>How-Many task</i> tests an understanding of the ability to count the number of objects in a given set. In the <i>Give-N task</i>, children are presented with a pile of items and asked to construct a subset of objects of a specific size. <i>Give-N tasks</i> are considered to be more difficult and a more comprehensive measure of cardinality understanding. However, despite the performance differences across the two tasks, covariance between them can be used to test whether they measure the same construct [<a href="https://www.sciencedirect.com/science/article/pii/S0885200623001151/">O'Rear et al., 2023</a>].
                In this post, I will do a deep dive into the performance of multi-modal text-vision models on these tasks. 
            </p>
             <!-- Two-image block -->
             <div class="image-block narrow">
                 <div class="image-container">
                     <img src="figs/orange_3.png" alt="3 oranges.">
                     <p class="caption"><strong>Figure 1a:</strong> Image generated by OmniGen2 for the prompt <strong>3 oranges.</strong></p>
                 </div>
                 <div class="image-container">
                     <img src="figs/sheep_3.png" alt="3 sheep.">
                     <p class="caption"><strong>Figure 1b:</strong> Image generated by OmniGen2 for the prompt <strong>3 sheep.</strong></p>
                 </div>
             </div>
            
            <p>
            To analyze the performance of models on this task, we first define a taxonomy categorizing the different countable noun forms based on morphological and semantic features.
            </p>
            <!-- Two-image block -->
            <div class="image-block">
                <div class="image-container">
                    <img src="figs/morphological_taxonomy.png" alt="Morphological taxonomy.">
                    <p class="caption"><strong>Figure 2a:</strong> Taxonomy of countable noun forms based on morphological features.</p>
                </div>
                <div class="image-container">
                    <img src="figs/semantic_taxonomy.png" alt="Semantic taxonomy.">
                    <p class="caption"><strong>Figure 2b:</strong> Taxonomy of countable noun forms based on semantic features (under construction).</p>
                </div>
            </div>
            

            <h2>Related Work</h2>

            <p>
                Recent work evaluating generative models for counting skills have found that vision-text models possess <i>some</i> ability to generate images with the correct number of objects (<i>Give-n</i> task), but can struggle when presented with unusual descriptive modifiers [<a href="https://cocosci.princeton.edu/papers/rane2024count.pdf">Rane et al. (2024)</a>]. Further, their performance degrades rapidly for cardinalities greater than 5. Other works have investigated the ability of vision-text models to count objects in the wild [<a href="https://arxiv.org/pdf/1604.03505">Ma et al. (2025)</a>, <a href="https://arxiv.org/pdf/1810.12440">Acharya et al. (2024)</a>] (<i>How Many?</i> task).
            </p>

            <h2>Experimental Setup</h2>

            <p>
                We evaluate the OmniGen2 model on the following tasks to understand numerical cognition of simple countable noun entities in multi-modal text-vision models: 
                <ol>
                    <li>
                        <i>Give-n</i> task: We prompt models with a phrase or instruction to generate an image of a pre-specified count of a countable noun entity . </li>
                    <li>
                        <i>How-Many</i> task: Given a synthetically generated image with single abstract entities, we prompt models to output the cardinality of the entity set in the given image. </li>
                </ol>

            </p>

            <h2>Results</h2>
            <p>
                As observed by [<a href="https://cocosci.princeton.edu/papers/rane2024count.pdf"></a>Rane et al. (2024)</a>], we find that models exhibit moderate success in generating images of entities with lower cardinalities (1-3). However, the performance degrades rapidly as we increase <i>n</i> to 4 or greater. The following figures show the accuracies, error rates, and distribution counts on the Give-n task. We make the following observations:
                <ol>
                    <li>With higher cardinalities, the model tends to over-count than under-count (i.e., generate more objects than the input prompt count).</li>
                    <li>A bias towards generating objects of counts 9.</li>
                    <li>Certain entities are more difficult to count than others (e.g. oranges).</li>
                </ol>

            </p>
            <div class="image-grid">
                <div class="image-container">
                    <img src="figs/acc.png" alt="Accuracy of model-generated counts.">
                        <p class="caption"><strong>Figure 3c:</strong> Distribution of model-generated counts on the Give-n task.</p>
                    </div>
                <div class="image-container">
                    <img src="figs/error.png" alt="Error rates on Give-n task.">
                    <p class="caption"><strong>Figure 3a:</strong> Differences between the average model generated count and the input prompt count on the Give-n task.</p>
                </div>
                <div class="image-container">
                    <img src="figs/ridge_plot.png" alt="Give-n count Distribution.">
                    <p class="caption"><strong>Figure 3b:</strong> Distribution of model-generated counts on the Give-n task.</p>
                </div>
            </div>
        </article>
    </div>
</body>
</html>